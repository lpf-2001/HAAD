import os
import re
import sys
import torch
import datetime
import time
from configobj import ConfigObj
import pytz
import torch.nn as nn
from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
from model import *

# è‡ªå®šä¹‰æ¨¡å—è·¯å¾„
parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '../utils'))
sys.path.append(parent_dir)

from data import *

# ==========================================
# è¾…åŠ©å‡½æ•°
# ==========================================
def split_alpha_number(s):
    m = re.match(r'^([^\d]+)(\d+)$', s)
    return (m.group(1), int(m.group(2))) if m else (s, None)



def curtime():
    tz = pytz.timezone('Asia/Chongqing')
    return datetime.datetime.now(tz).strftime('%H:%M:%S')

def log_info(dataset, num_classes, model_name, msg, id=None):
    print(f"> {msg}")
    log_path = f'../utils/trained_model/{dataset}{num_classes}/{dataset}_{model_name}.out'
    with open(log_path, "a") as f:
        prefix = f"ID {id} {curtime()}>\t" if id else ""
        f.write(f"{prefix}{msg}\n")
        
        
        
# ==========================================
# æ•°æ®åŠ è½½
# ==========================================
def load_datasets(data_name, num_classes, learn_param):
    val_ratio = learn_param.as_float('val_ratio')
    test_ratio = learn_param.as_float('test_ratio')

    if data_name == "sirinam":
        loader = LoadDataNoDefCW
    elif data_name == "rimmer":
        loader = load_rimmer_dataset
    else:
        raise ValueError(f"Unknown dataset name: {data_name}")

    X_train, y_train, X_valid, y_valid, X_test, y_test = loader(
        input_size=200, num_classes=num_classes, val_ratio=val_ratio, test_ratio=test_ratio
    )

    print(f"Train: {X_train.shape}, Valid: {X_valid.shape}, Test: {X_test.shape}")

    batch_size = learn_param.as_int('batch_size')
    return (
        DataLoader(MyDataset(X_train, y_train), batch_size=batch_size, shuffle=True),
        DataLoader(MyDataset(X_valid, y_valid), batch_size=batch_size, shuffle=False),
        DataLoader(MyDataset(X_test, y_test), batch_size=batch_size, shuffle=False),
    )



# ==========================================
# æ¨¡å‹æ„å»º
# ==========================================
def build_model_instance(model_type, dataset, config):
    data_name, num_classes = split_alpha_number(dataset)
    if model_type == "cnn":
        return Tor_cnn(200, num_classes)
    elif model_type == "df":
        return DFNet(num_classes)
    elif model_type == "varcnn":
        return VarCNN(200, num_classes)
    elif model_type == "lstm":
        mp = config['lstm']['model_param']
        return Tor_lstm(
            input_size=mp.as_int('input_size'),
            hidden_size=mp.as_int('hidden_size'),
            num_layers=mp.as_int('num_layers'),
            num_classes=num_classes
        )
    elif model_type == "sdae":
        layers = [config[str(i)] for i in range(1, config.as_int('nb_layers') + 1)]
        config['layers'] = layers
        return build_model(
            learn_params=config, train_gen=None, test_gen=None,
            steps=config.as_int('batch_size'), nb_classes=num_classes
        )
    elif model_type == "awf":
        return AWFNet(num_classes=num_classes)
    elif model_type == "ensemble":
        m1 = "varcnn"
        m2 = "lstm"
        m3 = "df"
        Model1 = VarCNN(200, num_classes).to(device)
        Model2= Tor_lstm(
            input_size=config['lstm']['model_param'].as_int('input_size'),
            hidden_size=config['lstm']['model_param'].as_int('hidden_size'),
            num_layers=config['lstm']['model_param'].as_int('num_layers'),
            num_classes=num_classes
        ).to(device)
        Model3 = DFNet(num_classes=num_classes).to(device)
        
        base_dir = f"../utils/trained_model/{dataset}"
        for model_obj, fname in zip([Model1, Model2, Model3],
                                    [f"{m1}.pkl", f"{m2}.pkl", f"{m3}.pkl"]):
            path = os.path.join(base_dir, fname)
            if os.path.exists(path):
                model_obj.load_state_dict(torch.load(path, map_location=device))
                print(f"âœ… Loaded {fname} weights from {path}")
            else:
                print(f"âš  {fname} not found â€” using random init")
            
        
        
        return Tor_ensemble_model(Model1, Model2, Model3).to(device)
    else:
        raise ValueError(f"Unknown model type: {model_type}")
    
    
    
# ==========================================
# è®­ç»ƒä¸éªŒè¯
# ==========================================
def evaluate(model, dataloader, criterion):
    model.eval()
    total_loss, total_acc = 0, 0
    with torch.no_grad():
        for x, y in dataloader:
            x, y = x.float().to(device), y.float().to(device)
            outputs = model(x)
            loss = criterion(outputs, y.argmax(1))
            total_loss += loss.item()
            total_acc += (outputs.argmax(1) == y.argmax(1)).sum().item()
    acc = total_acc / len(dataloader.dataset)
    return total_loss / len(dataloader), acc

def test_metrics(model, dataloader):
    model.eval()
    acc, recall, precision, f1 = 0, 0, 0, 0
    with torch.no_grad():
        for x, y in dataloader:
            x, y = x.float().to(device), y.float().to(device)
            preds = model(x).argmax(1).cpu().numpy()
            labels = y.argmax(1).cpu().numpy()
            acc += accuracy_score(labels, preds)
            recall += recall_score(labels, preds, average='weighted')
            precision += precision_score(labels, preds, average='weighted')
            f1 += f1_score(labels, preds, average='weighted')
    n = len(dataloader)
    return acc/n, recall/n, precision/n, f1/n


def analyze_model_performance(model, dataloader, dataset_name, num_classes, model_name, 
                              train_time=None, device='cuda', log_path=None, 
                              warmup=10, repeat=100):
    """
    åˆ†ææ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ä¸éƒ¨ç½²å¼€é”€
    """
    model.eval()
    model.to(device)

    # === 1ï¸âƒ£ æ¨¡å‹å‚æ•°ç»Ÿè®¡ ===
    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    torch.save(model.state_dict(), "temp_model.pth")
    model_size_mb = os.path.getsize("temp_model.pth") / (1024 * 1024)
    os.remove("temp_model.pth")

    # === 2ï¸âƒ£ æ¨ç†å»¶è¿Ÿç»Ÿè®¡ ===
    dummy_input = next(iter(dataloader))[0].float().to(device)[:1]  # å–ä¸€ä¸ªæ ·æœ¬
    with torch.no_grad():
        for _ in range(warmup):  # é¢„çƒ­
            _ = model(dummy_input)

        torch.cuda.synchronize()
        start = time.time()
        for _ in range(repeat):
            _ = model(dummy_input)
        torch.cuda.synchronize()
        end = time.time()
    
    avg_latency = (end - start) / repeat
    throughput = 1.0 / avg_latency

    # === 3ï¸âƒ£ æ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡ ===
    all_preds, all_labels = [], []
    with torch.no_grad():
        for x, y in dataloader:
            x, y = x.float().to(device), y.float().to(device)
            preds = model(x).argmax(1).cpu().numpy()
            labels = y.argmax(1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels)

    acc = accuracy_score(all_labels, all_preds)
    recall = recall_score(all_labels, all_preds, average='weighted')
    precision = precision_score(all_labels, all_preds, average='weighted')
    f1 = f1_score(all_labels, all_preds, average='weighted')

    # === 4ï¸âƒ£ è¾“å‡ºæŠ¥å‘Š ===
    report = (
    f"\nğŸ“Š ====== Model Performance Summary ======\n"
    f"ğŸ“ Dataset: {dataset_name}{num_classes}\n"
    f"ğŸ§  Model: {model_name}\n"
    f"-------------------------------------------\n"
    f"ğŸ”¢ Trainable Params: {num_params:,}\n"
    f"ğŸ’¾ Model Size: {model_size_mb:.2f} MB\n"
    f"âš™ï¸  Avg Inference Latency: {avg_latency * 1000:.3f} ms\n"
    f"ğŸš€ Throughput: {throughput:.2f} samples/s\n"
    )

    if train_time:
        report += f"ğŸ•’ Training Time: {train_time:.2f}s\n"

    report += (
        f"-------------------------------------------\n"
        f"âœ… Accuracy:  {acc:.4f}\n"
        f"ğŸ“ˆ Recall:    {recall:.4f}\n"
        f"ğŸ¯ Precision: {precision:.4f}\n"
        f"ğŸ† F1-score:  {f1:.4f}\n"
        f"===========================================\n"
    )


    print(report)

    # === 5ï¸âƒ£ å†™å…¥æ—¥å¿—æ–‡ä»¶ ===
    if log_path is None:
        log_path = f"../utils/trained_model/{dataset_name}{num_classes}/{dataset_name}_{model_name}_summary.out"
    with open(log_path, "a") as f:
        f.write(report + "\n")

    return {
        "params": num_params,
        "size_MB": model_size_mb,
        "latency_ms": avg_latency * 1000,
        "throughput": throughput,
        "acc": acc,
        "recall": recall,
        "precision": precision,
        "f1": f1,
        "train_time": train_time,
    }


def evaluate_ensemble_vs_single(m1, m2, m3, dataname, num_classes, dataloader, device='cuda'):
    
    
    
    
    config = ConfigObj("My_tor.conf")
    learn_param = config['ensemble']

    Model1 = VarCNN(200, num_classes).to(device)
    Model2= Tor_lstm(
        input_size=config['lstm']['model_param'].as_int('input_size'),
        hidden_size=config['lstm']['model_param'].as_int('hidden_size'),
        num_layers=config['lstm']['model_param'].as_int('num_layers'),
        num_classes=num_classes
    ).to(device)
    Model3 = DFNet(num_classes=num_classes).to(device)
    
    base_dir = f"../utils/trained_model/{dataname}{num_classes}"
    for model_obj, fname in zip([Model1, Model2, Model3],
                                [f"{m1}.pkl", f"{m2}.pkl", f"{m3}.pkl"]):
        path = os.path.join(base_dir, fname)
        if os.path.exists(path):
            model_obj.load_state_dict(torch.load(path, map_location=device))
            print(f"âœ… Loaded {fname} weights from {path}")
        else:
            print(f"âš  {fname} not found â€” using random init")


    

    # 5ï¸âƒ£ æ„å»ºåŠ æƒæŠ•ç¥¨ ensemble
    ensemble = build_model_instance('ensemble',dataname+str(num_classes),config).to(device)


    ensemble.load_state_dict(torch.load(f"../utils/trained_model/{dataname}{num_classes}/ensemble.pkl", map_location=device))
    
    
    ensemble.eval()
    Model1.eval()
    Model2.eval()
    Model3.eval()

    total = 0
    correct_ens = 0
    correct_m1 = 0
    correct_m2 = 0
    correct_m3 = 0
    fix_count = 0   # å•æ¨¡å‹é”™ï¼Œé›†æˆæ”¹æ­£
    degrade_count = 0 # å•æ¨¡å‹å¯¹ï¼Œé›†æˆåè€Œé”™

    with torch.no_grad():
        for x, y in dataloader:
            x, y = x.to(device), y.argmax(-1).to(device)

            out_ens = ensemble(x)                   # é›†æˆè¾“å‡º
            out1 = Model1(x)
            out2 = Model2(x)
            out3 = Model3(x)

            pred_ens = out_ens.argmax(dim=-1)
            p1 = out1.argmax(dim=-1)
            p2 = out2.argmax(dim=-1)
            p3 = out3.argmax(dim=-1)

            # æœ€å¼ºå•æ¨¡å‹ï¼ˆä»ä¸‰ä¸ªä¸­é€‰ä¸€ä¸ªä½ è§‰å¾—æœ€å¼ºçš„ï¼Œä¹Ÿå¯é€æ ·æœ¬é€‰æœ€å¤§ç½®ä¿¡åº¦ï¼‰
            # è¿™é‡Œç®€å•ç”¨ VarCNN ä½œä¸º baseline
            pred_best = p3    

            # ç»Ÿè®¡æ­£ç¡®æ•°
            correct_ens += (pred_ens == y).sum().item()
            correct_m1 += (p1 == y).sum().item()
            correct_m2 += (p2 == y).sum().item()
            correct_m3 += (p3 == y).sum().item()

            # æ ·æœ¬çº§æ¯”è¾ƒ
            for i in range(len(y)):
                if pred_best[i] != y[i] and pred_ens[i] == y[i]:
                    fix_count += 1
                if pred_best[i] == y[i] and pred_ens[i] != y[i]:
                    degrade_count += 1

            total += y.size(0)
    log_info(dataname, num_classes, "ensemble_vs_single", "===== âœ… é›†æˆ vs å•æ¨¡å‹ å¯¹æ¯”ç»“æœ =====")
    log_info(dataname, num_classes, "ensemble_vs_single", f"âœ” Dataset: {dataname}{num_classes}")
    log_info(dataname, num_classes, "ensemble_vs_single", f"âœ” Ensemble  Acc: {correct_ens/total:.4f}")
    log_info(dataname, num_classes, "ensemble_vs_single", f"âœ” {m1}    Acc: {correct_m1/total:.4f}")
    log_info(dataname, num_classes, "ensemble_vs_single", f"âœ” {m2}    Acc: {correct_m2/total:.4f}")
    log_info(dataname, num_classes, "ensemble_vs_single", f"âœ” {m3}      Acc: {correct_m3/total:.4f}")
    log_info(dataname, num_classes, "ensemble_vs_single", "--------------------------------------")
    log_info(dataname, num_classes, "ensemble_vs_single", f"ğŸ”¹ ä¿®æ­£é”™è¯¯æ ·æœ¬æ•° (å•æ¨¡å‹é”™ï¼Œä½†é›†æˆå¯¹) : {fix_count}")
    log_info(dataname, num_classes, "ensemble_vs_single", f"ğŸ”¸ é€€åŒ–æ ·æœ¬æ•°   (å•æ¨¡å‹å¯¹ï¼Œä½†é›†æˆé”™) : {degrade_count}")
    log_info(dataname, num_classes, "ensemble_vs_single", f"ğŸ”¹ æå‡æ¯”ä¾‹: {fix_count / total:.4f}")
    log_info(dataname, num_classes, "ensemble_vs_single", f"ğŸ”¸ é€€åŒ–æ¯”ä¾‹: {degrade_count / total:.4f}")


    print("===== âœ… é›†æˆ vs å•æ¨¡å‹ å¯¹æ¯”ç»“æœ =====")
    print(f"âœ” Ensemble  Acc: {correct_ens/total:.4f}")
    print(f"âœ” {m1}    Acc: {correct_m1/total:.4f}")
    print(f"âœ” {m2}    Acc: {correct_m2/total:.4f}")
    print(f"âœ” {m3}      Acc: {correct_m3/total:.4f}")
    print("--------------------------------------")
    print(f"ğŸ”¹ ä¿®æ­£é”™è¯¯æ ·æœ¬æ•° (å•æ¨¡å‹é”™ï¼Œä½†é›†æˆå¯¹) : {fix_count}")
    print(f"ğŸ”¸ é€€åŒ–æ ·æœ¬æ•°   (å•æ¨¡å‹å¯¹ï¼Œä½†é›†æˆé”™) : {degrade_count}")
    print(f"ğŸ”¹ æå‡æ¯”ä¾‹: {fix_count / total:.4f}")
    print(f"ğŸ”¸ é€€åŒ–æ¯”ä¾‹: {degrade_count / total:.4f}")
